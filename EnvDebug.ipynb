{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baha/anaconda3/envs/gym-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "pybullet build time: Feb  1 2023 20:12:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robotiqGymEnv __init__\n",
      "robot base reset\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RKL GT1)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 22.2.5\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 22.2.5\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RKL GT1)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "robot base reset\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys\n",
    "import gymnasium\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "import pybullet as p\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import A2C, DDPG, PPO, TD3, SAC\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from robotiqGymEnv import robotiqGymEnv\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    env = robotiqGymEnv(records=True, renders=True)\n",
    "\n",
    "    dir = \"models/20230316-03:42PM_SAC_M10000_0.04_39/best_model.zip\"\n",
    "    model = SAC.load(dir)\n",
    "\n",
    "    # mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    # print(mean_reward)\n",
    "\n",
    "    t_ = np.array([])\n",
    "    rewards_ = np.array([])\n",
    "    xaction_ = np.array([])\n",
    "    yaction_ = np.array([])\n",
    "    zaction_ = np.array([])\n",
    "    xposition_ = np.array([])\n",
    "    yposition_ = np.array([])\n",
    "    zposition_ = np.array([])\n",
    "    gripperroll_ = np.array([])\n",
    "    gripperpitch_ = np.array([])\n",
    "    gripperyaw_ = np.array([])\n",
    "    targetxposition_ = np.array([])\n",
    "    targetyposition_ = np.array([])\n",
    "    targetzposition_ = np.array([])\n",
    "    targetroll_ = np.array([])\n",
    "    targetpitch_ = np.array([])\n",
    "    targetyaw_ = np.array([])\n",
    "    closestpoint_ = np.array([])\n",
    "    contactforce_ = np.array([])\n",
    "\n",
    "\n",
    "    dones = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not dones:\n",
    "        t_ = np.append(t_, env._envStepCounter)\n",
    "        \n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "\n",
    "        rewards_ = np.append(rewards_,env._reward())\n",
    "        xaction_ = np.append(xaction_,action[0])\n",
    "        yaction_ = np.append(yaction_,action[1])\n",
    "        zaction_ = np.append(zaction_,action[2])\n",
    "        xposition_ = np.append(xposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][0])\n",
    "        yposition_ = np.append(yposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][1])\n",
    "        zposition_ = np.append(zposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][2])\n",
    "        gripperangle = p.getEulerFromQuaternion(p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[1])\n",
    "        gripperroll_ = np.append(gripperroll_ , gripperangle[0])\n",
    "        gripperpitch_ = np.append(gripperpitch_ , gripperangle[1])\n",
    "        gripperyaw_ = np.append(gripperyaw_ , gripperangle[2])\n",
    "\n",
    "        targetxposition_ = np.append(targetxposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][0])\n",
    "        targetyposition_ = np.append(targetyposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][1])\n",
    "        targetzposition_ = np.append(targetzposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][2])\n",
    "        targetangle = p.getEulerFromQuaternion(p.getBasePositionAndOrientation(env.blockUid)[1])\n",
    "        targetroll_ = np.append(targetroll_ , targetangle[0])\n",
    "        targetpitch_ = np.append(targetpitch_ , targetangle[1])\n",
    "        targetyaw_ = np.append(targetyaw_ , targetangle[2])\n",
    "\n",
    "        closestpoint_ = np.append(closestpoint_ , p.getClosestPoints(env._robotiq.robotiqUid, env.blockUid, 100, -1, -1)[0][8])\n",
    "        xtargetvel = p.getBaseVelocity(env.blockUid)[0][0]\n",
    "        ytargetvel = p.getBaseVelocity(env.blockUid)[0][1]\n",
    "        ztargetvel = p.getBaseVelocity(env.blockUid)[0][2]\n",
    "        contactforce_ = np.append(contactforce_ , env._contactinfo()[4])\n",
    "        env.render()\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "\n",
    "    print(\"xvel: \", xtargetvel)\n",
    "    print(\"yvel: \", ytargetvel)\n",
    "    print(\"zvel: \", ztargetvel)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xaction_, label=\"x action\")\n",
    "    plt.plot(yaction_, label=\"y action\")\n",
    "    plt.plot(zaction_, label=\"z action\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(rewards_, label=\"reward\")\n",
    "    plt.title(\"Reward\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xposition_, label=\"x position\", color='r')\n",
    "    plt.plot(yposition_, label=\"y position\", color='g')\n",
    "    plt.plot(zposition_, label=\"z position\", color='b')\n",
    "\n",
    "    plt.plot(targetxposition_, label=\"target x position\", linestyle='--', color='r')\n",
    "    plt.plot(targetyposition_, label=\"target y position\", linestyle='--', color='g')\n",
    "    plt.plot(targetzposition_, label=\"target z position\", linestyle='--', color='b')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(gripperroll_, label=\"gripper roll\")\n",
    "    plt.plot(gripperpitch_, label=\"gripper pitch\")\n",
    "    plt.plot(gripperyaw_, label=\"gripper yaw\")\n",
    "    plt.plot(targetroll_, label=\"target roll\", linestyle='--')\n",
    "    plt.plot(targetpitch_, label=\"target pitch\", linestyle='--')\n",
    "    plt.plot(targetyaw_, label=\"target yaw\", linestyle='--')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(closestpoint_, label=\"closest distance\")\n",
    "    plt.plot(np.zeros(len(closestpoint_)), linestyle='--')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(contactforce_, label=\"contact force\")\n",
    "    plt.legend()\n",
    "  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f48a7e74faba9c83f9b785a217b5868eed225abe1717429cf95272f7d5045b20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
