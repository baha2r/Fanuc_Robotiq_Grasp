{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baha/anaconda3/envs/gym-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "pybullet build time: Feb  1 2023 23:25:41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robotiqGymEnv __init__\n",
      "robot base reset\n",
      "robot base reset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape (39,) for Box environment, please use (45,) or (n_env, 45) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m\n\u001b[1;32m    115\u001b[0m     plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mTotal Force\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m   main()\n",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m t_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(t_, env\u001b[39m.\u001b[39m_envStepCounter)\n\u001b[1;32m     52\u001b[0m \u001b[39m#   action = [0 , 0 , 0 , 0 , 0 , 0 , 0.1]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m#   action = [0 , 0 , 0 , 1 , 0 , 1 , 1]\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m action, _states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(obs, deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     56\u001b[0m \u001b[39m# action = [0, 0, 0, 0, 0, 0]\u001b[39;00m\n\u001b[1;32m     57\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:535\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    521\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/policies.py:340\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m# if state is None:\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m#     state = self.initial_state\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m# if episode_start is None:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_training_mode(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 340\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_to_tensor(observation)\n\u001b[1;32m    342\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    343\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/policies.py:255\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    251\u001b[0m     observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(observation)\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[39m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     vectorized_env \u001b[39m=\u001b[39m is_vectorized_observation(observation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space)\n\u001b[1;32m    256\u001b[0m     \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/utils.py:380\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[39min\u001b[39;00m is_vec_obs_func_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 380\u001b[0m         \u001b[39mreturn\u001b[39;00m is_vec_obs_func(observation, observation_space)\n\u001b[1;32m    381\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[39m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/utils.py:247\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Unexpected observation shape \u001b[39m\u001b[39m{\u001b[39;00mobservation\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBox environment, please use \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mor (n_env, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) for the observation shape.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, observation_space\u001b[39m.\u001b[39mshape)))\n\u001b[1;32m    251\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Unexpected observation shape (39,) for Box environment, please use (45,) or (n_env, 45) for the observation shape."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys\n",
    "import gymnasium\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "import pybullet as p\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import A2C, DDPG, PPO, TD3, SAC\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from robotiqGymEnv import robotiqGymEnv\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    env = robotiqGymEnv(records=False, renders=False)\n",
    "\n",
    "    dir = \"models/20230307-10:21AM_SAC_M1000_0.04_wovel_long/best_model.zip\"\n",
    "    model = SAC.load(dir)\n",
    "\n",
    "    # mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    # print(mean_reward)\n",
    "\n",
    "    t_ = np.array([])\n",
    "    rewards_ = np.array([])\n",
    "    xaction_ = np.array([])\n",
    "    yaction_ = np.array([])\n",
    "    zaction_ = np.array([])\n",
    "    xposition_ = np.array([])\n",
    "    yposition_ = np.array([])\n",
    "    zposition_ = np.array([])\n",
    "    targetxposition_ = np.array([])\n",
    "    targetyposition_ = np.array([])\n",
    "    targetzposition_ = np.array([])\n",
    "    closestpoint_ = np.array([])\n",
    "    totalforce_ = np.array([])\n",
    "    totalforce_2 = np.array([])\n",
    "\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        t_ = np.append(t_, env._envStepCounter)\n",
    "        #   action = [0 , 0 , 0 , 0 , 0 , 0 , 0.1]\n",
    "        # else:\n",
    "        #   action = [0 , 0 , 0 , 1 , 0 , 1 , 1]\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        # action = [0, 0, 0, 0, 0, 0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        rewards_ = np.append(rewards_,reward)\n",
    "        xaction_ = np.append(xaction_,action[0])\n",
    "        yaction_ = np.append(yaction_,action[1])\n",
    "        zaction_ = np.append(zaction_,action[2])\n",
    "        xposition_ = np.append(xposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][0])\n",
    "        yposition_ = np.append(yposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][1])\n",
    "        zposition_ = np.append(zposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][2])\n",
    "        targetxposition_ = np.append(targetxposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][0])\n",
    "        targetyposition_ = np.append(targetyposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][1] - 0.2)\n",
    "        targetzposition_ = np.append(targetzposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][2])\n",
    "        closestpoint_ = np.append(closestpoint_ , p.getClosestPoints(env._robotiq.robotiqUid, env.blockUid, 100, -1, -1)[0][8])\n",
    "        xtargetvel = p.getBaseVelocity(env.blockUid)[0][0]\n",
    "        ytargetvel = p.getBaseVelocity(env.blockUid)[0][1]\n",
    "        ztargetvel = p.getBaseVelocity(env.blockUid)[0][2]\n",
    "        totalforce_ = np.append(totalforce_,env._contactinfo()[4])\n",
    "        totalforce_2 = np.append(totalforce_2,p.getClosestPoints(env.blockUid, env._robotiq.robotiqUid, 100, -1, -1)[0][9])\n",
    "        # env.render()\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "\n",
    "    plt.plot(xaction_, label=\"x action\")\n",
    "    plt.plot(yaction_, label=\"y action\")\n",
    "    plt.plot(zaction_, label=\"z action\")\n",
    "    plt.xlabel(\"timestep\")\n",
    "    plt.legend()\n",
    "\n",
    "    print(\"xvel: \", xtargetvel)\n",
    "    print(\"yvel: \", ytargetvel)\n",
    "    print(\"zvel: \", ztargetvel)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(rewards_, label=\"reward\")\n",
    "    plt.xlabel(\"timestep\")\n",
    "    plt.title(\"Reward\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xposition_, label=\"x\", color='red')\n",
    "    plt.plot(yposition_, label=\"y\", color='green')\n",
    "    plt.plot(zposition_, label=\"z\", color='blue')\n",
    "\n",
    "    plt.plot(targetxposition_, linestyle='--', color='red')\n",
    "    plt.plot(targetyposition_, linestyle='--', color='green')\n",
    "    plt.plot(targetzposition_, linestyle='--', color='blue')\n",
    "    plt.xlabel(\"timestep\")\n",
    "    plt.title(\"Position\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(closestpoint_, label=\"closest distance\")\n",
    "    plt.plot(np.zeros(len(closestpoint_)), linestyle='--')\n",
    "    plt.xlabel(\"timestep\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(totalforce_, label=\"total force\")\n",
    "    plt.xlabel(\"timestep\")\n",
    "    plt.title(\"Total Force\")\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f48a7e74faba9c83f9b785a217b5868eed225abe1717429cf95272f7d5045b20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
