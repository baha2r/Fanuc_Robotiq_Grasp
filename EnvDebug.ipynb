{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baha/anaconda3/envs/gym-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "pybullet build time: Feb  1 2023 20:12:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robotiqGymEnv __init__\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RKL GT1)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 22.2.5\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 22.2.5\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RKL GT1)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "robot base reset\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "robot base reset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape (39,) for Box environment, please use (45,) or (n_env, 45) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 145\u001b[0m\n\u001b[1;32m    140\u001b[0m     plt\u001b[39m.\u001b[39mlegend()\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m   main()\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m dones:\n\u001b[1;32m     61\u001b[0m     t_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(t_, env\u001b[39m.\u001b[39m_envStepCounter)\n\u001b[0;32m---> 63\u001b[0m     action, _states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(obs, deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     64\u001b[0m     obs, rewards, dones, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     66\u001b[0m     rewards_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(rewards_,env\u001b[39m.\u001b[39m_reward())\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:535\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    521\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/policies.py:340\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m# if state is None:\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m#     state = self.initial_state\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m# if episode_start is None:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_training_mode(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 340\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_to_tensor(observation)\n\u001b[1;32m    342\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    343\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/policies.py:255\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    251\u001b[0m     observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(observation)\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[39m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     vectorized_env \u001b[39m=\u001b[39m is_vectorized_observation(observation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space)\n\u001b[1;32m    256\u001b[0m     \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/utils.py:380\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[39min\u001b[39;00m is_vec_obs_func_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 380\u001b[0m         \u001b[39mreturn\u001b[39;00m is_vec_obs_func(observation, observation_space)\n\u001b[1;32m    381\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[39m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gym-env/lib/python3.10/site-packages/stable_baselines3/common/utils.py:247\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Unexpected observation shape \u001b[39m\u001b[39m{\u001b[39;00mobservation\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBox environment, please use \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mor (n_env, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) for the observation shape.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, observation_space\u001b[39m.\u001b[39mshape)))\n\u001b[1;32m    251\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Unexpected observation shape (39,) for Box environment, please use (45,) or (n_env, 45) for the observation shape."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys\n",
    "import gymnasium\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "import pybullet as p\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import A2C, DDPG, PPO, TD3, SAC\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from robotiqGymEnv import robotiqGymEnv\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import csv\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    env = robotiqGymEnv(records=False, renders=True)\n",
    "\n",
    "    dir = \"models/20230312-06:53PM_SAC_M10000_0.04_WTS/best_model.zip\"\n",
    "    model = SAC.load(dir)\n",
    "\n",
    "    # mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    # print(mean_reward)\n",
    "\n",
    "    t_ = np.array([])\n",
    "    rewards_ = np.array([])\n",
    "    xaction_ = np.array([])\n",
    "    yaction_ = np.array([])\n",
    "    zaction_ = np.array([])\n",
    "    xposition_ = np.array([])\n",
    "    yposition_ = np.array([])\n",
    "    zposition_ = np.array([])\n",
    "    gripperroll_ = np.array([])\n",
    "    gripperpitch_ = np.array([])\n",
    "    gripperyaw_ = np.array([])\n",
    "    targetxposition_ = np.array([])\n",
    "    targetyposition_ = np.array([])\n",
    "    targetzposition_ = np.array([])\n",
    "    targetroll_ = np.array([])\n",
    "    targetpitch_ = np.array([])\n",
    "    targetyaw_ = np.array([])\n",
    "    closestpoint_ = np.array([])\n",
    "    contactforce_ = np.array([])\n",
    "\n",
    "\n",
    "    dones = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not dones:\n",
    "        t_ = np.append(t_, env._envStepCounter)\n",
    "        \n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "\n",
    "        rewards_ = np.append(rewards_,env._reward())\n",
    "        xaction_ = np.append(xaction_,action[0])\n",
    "        yaction_ = np.append(yaction_,action[1])\n",
    "        zaction_ = np.append(zaction_,action[2])\n",
    "        xposition_ = np.append(xposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][0])\n",
    "        yposition_ = np.append(yposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][1])\n",
    "        zposition_ = np.append(zposition_ , p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[0][2])\n",
    "        gripperangle = p.getEulerFromQuaternion(p.getBasePositionAndOrientation(env._robotiq.robotiqUid)[1])\n",
    "        gripperroll_ = np.append(gripperroll_ , gripperangle[0])\n",
    "        gripperpitch_ = np.append(gripperpitch_ , gripperangle[1])\n",
    "        gripperyaw_ = np.append(gripperyaw_ , gripperangle[2])\n",
    "\n",
    "        targetxposition_ = np.append(targetxposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][0])\n",
    "        targetyposition_ = np.append(targetyposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][1])\n",
    "        targetzposition_ = np.append(targetzposition_ , p.getBasePositionAndOrientation(env.blockUid)[0][2])\n",
    "        targetangle = p.getEulerFromQuaternion(p.getBasePositionAndOrientation(env.blockUid)[1])\n",
    "        targetroll_ = np.append(targetroll_ , targetangle[0])\n",
    "        targetpitch_ = np.append(targetpitch_ , targetangle[1])\n",
    "        targetyaw_ = np.append(targetyaw_ , targetangle[2])\n",
    "\n",
    "        closestpoint_ = np.append(closestpoint_ , p.getClosestPoints(env._robotiq.robotiqUid, env.blockUid, 100, -1, -1)[0][8])\n",
    "        xtargetvel = p.getBaseVelocity(env.blockUid)[0][0]\n",
    "        ytargetvel = p.getBaseVelocity(env.blockUid)[0][1]\n",
    "        ztargetvel = p.getBaseVelocity(env.blockUid)[0][2]\n",
    "        contactforce_ = np.append(contactforce_ , env._contactinfo()[4])\n",
    "        env.render()\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "\n",
    "    print(\"xvel: \", xtargetvel)\n",
    "    print(\"yvel: \", ytargetvel)\n",
    "    print(\"zvel: \", ztargetvel)\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xaction_, label=\"x action\")\n",
    "    plt.plot(yaction_, label=\"y action\")\n",
    "    plt.plot(zaction_, label=\"z action\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(rewards_, label=\"reward\")\n",
    "    plt.title(\"Reward\")\n",
    "    savetxt('Reward.csv', rewards_, delimiter=' ')\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xposition_, label=\"x position\", color='r')\n",
    "    plt.plot(yposition_, label=\"y position\", color='g')\n",
    "    plt.plot(zposition_, label=\"z position\", color='b')\n",
    "\n",
    "    plt.plot(targetxposition_, label=\"target x position\", linestyle='--', color='r')\n",
    "    plt.plot(targetyposition_, label=\"target y position\", linestyle='--', color='g')\n",
    "    plt.plot(targetzposition_, label=\"target z position\", linestyle='--', color='b')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(gripperroll_, label=\"gripper roll\")\n",
    "    plt.plot(gripperpitch_, label=\"gripper pitch\")\n",
    "    plt.plot(gripperyaw_, label=\"gripper yaw\")\n",
    "    plt.plot(targetroll_, label=\"target roll\", linestyle='--')\n",
    "    plt.plot(targetpitch_, label=\"target pitch\", linestyle='--')\n",
    "    plt.plot(targetyaw_, label=\"target yaw\", linestyle='--')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(closestpoint_, label=\"closest distance\")\n",
    "    plt.plot(np.zeros(len(closestpoint_)), linestyle='--')\n",
    "    plt.legend()\n",
    "    savetxt('closestpoint.csv', closestpoint_, delimiter=' ')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(contactforce_, label=\"contact force\")\n",
    "    plt.legend()\n",
    "  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f48a7e74faba9c83f9b785a217b5868eed225abe1717429cf95272f7d5045b20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
